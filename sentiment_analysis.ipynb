{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textstat import flesch_reading_ease, syllable_count,  lexicon_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\namit\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your text files are stored\n",
    "directory = (r'C:\\Users\\namit\\OneDrive\\Desktop\\blackCoffer\\new attempt\\extracted_articles')\n",
    "\n",
    "# Initialize a list to store the data for each text file\n",
    "data = []\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):  # Check if the file is a text file\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text_content = file.read()\n",
    "            \n",
    "            sentiment_scores = sia.polarity_scores(text_content)\n",
    "            positive_score = sentiment_scores['pos']\n",
    "            negative_score = sentiment_scores['neg']\n",
    "            polarity_score = sentiment_scores['compound']\n",
    "\n",
    "            reading_ease = flesch_reading_ease(text_content)\n",
    "\n",
    "            word_count = lexicon_count(text_content, removepunct=True)\n",
    "            sentence_count = text_content.count('.') + text_content.count('!') + text_content.count('?')\n",
    "            avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "            # complex_word_percentage = complex_word_count(text_content) / word_count * 100 if word_count > 0 else 0\n",
    "            # fog_index = 0.4 * (avg_sentence_length + complex_word_percentage)\n",
    "            avg_word_length = sum(len(word) for word in text_content.split()) / word_count if word_count > 0 else 0\n",
    "            syllable_per_word = syllable_count(text_content) / word_count if word_count > 0 else 0\n",
    "            personal_pronouns = sum(1 for word in nltk.word_tokenize(text_content) if word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'])\n",
    "            subjectivity_score = sentiment_scores['neu'] + sentiment_scores['pos'] + sentiment_scores['neg']\n",
    "            \n",
    "            # For now, let's print the filename and content to verify\n",
    "            data.append({\n",
    "                'URL_ID': filename[:-4],  # Remove the \".txt\" extension\n",
    "                'URL': '',  # Replace this with the URL if available\n",
    "                'POSITIVE SCORE': positive_score,\n",
    "                'NEGATIVE SCORE': negative_score,\n",
    "                'POLARITY SCORE': polarity_score,\n",
    "                'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "                'AVG SENTENCE LENGTH': avg_sentence_length,               \n",
    "                'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n",
    "                'WORD COUNT': word_count,\n",
    "                'SYLLABLE PER WORD': syllable_per_word,\n",
    "                'PERSONAL PRONOUNS': personal_pronouns,\n",
    "                'AVG WORD LENGTH': avg_word_length\n",
    "            })\n",
    "# Once you're done with the analysis, you can proceed to the next steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the order of columns as per your specified structure\n",
    "columns_order = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "                 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', \n",
    "                 'AVG NUMBER OF WORDS PER SENTENCE',  'WORD COUNT',\n",
    "                 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
    "\n",
    "# Reorder columns\n",
    "df = df[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td></td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>1.000</td>\n",
       "      <td>15.683544</td>\n",
       "      <td>15.683544</td>\n",
       "      <td>1239</td>\n",
       "      <td>1.476998</td>\n",
       "      <td>17</td>\n",
       "      <td>4.713479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td></td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>18.234568</td>\n",
       "      <td>18.234568</td>\n",
       "      <td>1477</td>\n",
       "      <td>1.748815</td>\n",
       "      <td>10</td>\n",
       "      <td>5.633040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td></td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>1.000</td>\n",
       "      <td>19.196429</td>\n",
       "      <td>19.196429</td>\n",
       "      <td>1075</td>\n",
       "      <td>2.017674</td>\n",
       "      <td>20</td>\n",
       "      <td>6.291163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td></td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.9950</td>\n",
       "      <td>1.000</td>\n",
       "      <td>20.843137</td>\n",
       "      <td>20.843137</td>\n",
       "      <td>1063</td>\n",
       "      <td>1.934149</td>\n",
       "      <td>7</td>\n",
       "      <td>6.121355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td></td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>1.001</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>692</td>\n",
       "      <td>1.723988</td>\n",
       "      <td>6</td>\n",
       "      <td>5.677746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td></td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.9986</td>\n",
       "      <td>1.000</td>\n",
       "      <td>21.673077</td>\n",
       "      <td>21.673077</td>\n",
       "      <td>1127</td>\n",
       "      <td>1.673469</td>\n",
       "      <td>3</td>\n",
       "      <td>5.368234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td></td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>1.000</td>\n",
       "      <td>27.871795</td>\n",
       "      <td>27.871795</td>\n",
       "      <td>1087</td>\n",
       "      <td>1.500460</td>\n",
       "      <td>13</td>\n",
       "      <td>4.840846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td></td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>1.000</td>\n",
       "      <td>16.291667</td>\n",
       "      <td>16.291667</td>\n",
       "      <td>391</td>\n",
       "      <td>1.767263</td>\n",
       "      <td>1</td>\n",
       "      <td>5.810742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td></td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.111111</td>\n",
       "      <td>17.111111</td>\n",
       "      <td>616</td>\n",
       "      <td>1.480519</td>\n",
       "      <td>5</td>\n",
       "      <td>4.883117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td></td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.9964</td>\n",
       "      <td>1.000</td>\n",
       "      <td>29.771429</td>\n",
       "      <td>29.771429</td>\n",
       "      <td>1042</td>\n",
       "      <td>1.672745</td>\n",
       "      <td>5</td>\n",
       "      <td>5.492322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID URL  POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  \\\n",
       "0   blackassign0001               0.137           0.020          0.9991   \n",
       "1   blackassign0002               0.149           0.036          0.9994   \n",
       "2   blackassign0003               0.118           0.051          0.9971   \n",
       "3   blackassign0004               0.102           0.145         -0.9950   \n",
       "4   blackassign0005               0.107           0.005          0.9955   \n",
       "..              ...  ..             ...             ...             ...   \n",
       "93  blackassign0096               0.074           0.150         -0.9986   \n",
       "94  blackassign0097               0.097           0.096         -0.3939   \n",
       "95  blackassign0098               0.098           0.019          0.9800   \n",
       "96  blackassign0099               0.138           0.033          0.9952   \n",
       "97  blackassign0100               0.089           0.134         -0.9964   \n",
       "\n",
       "    SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                1.000            15.683544                         15.683544   \n",
       "1                1.000            18.234568                         18.234568   \n",
       "2                1.000            19.196429                         19.196429   \n",
       "3                1.000            20.843137                         20.843137   \n",
       "4                1.001            17.300000                         17.300000   \n",
       "..                 ...                  ...                               ...   \n",
       "93               1.000            21.673077                         21.673077   \n",
       "94               1.000            27.871795                         27.871795   \n",
       "95               1.000            16.291667                         16.291667   \n",
       "96               1.000            17.111111                         17.111111   \n",
       "97               1.000            29.771429                         29.771429   \n",
       "\n",
       "    WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0         1239           1.476998                 17         4.713479  \n",
       "1         1477           1.748815                 10         5.633040  \n",
       "2         1075           2.017674                 20         6.291163  \n",
       "3         1063           1.934149                  7         6.121355  \n",
       "4          692           1.723988                  6         5.677746  \n",
       "..         ...                ...                ...              ...  \n",
       "93        1127           1.673469                  3         5.368234  \n",
       "94        1087           1.500460                 13         4.840846  \n",
       "95         391           1.767263                  1         5.810742  \n",
       "96         616           1.480519                  5         4.883117  \n",
       "97        1042           1.672745                  5         5.492322  \n",
       "\n",
       "[98 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
